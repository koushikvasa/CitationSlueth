{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup\n",
        "Installs the necessary Python libraries required to build the entire pipeline:\n",
        "* **PyMuPDF (fitz):** For parsing and extracting raw text from PDF files.\n",
        "* **SpaCy:** An industrial-strength NLP library used for identifying entities (like names and dates) and extracting grammatical relationships (triples).\n",
        "* **Neo4j:** The official driver that allows Python to connect to and manipulate your Neo4j AuraDB graph database.\n",
        "* **Sentence-Transformers:** Used to convert text chunks into numerical vectors (embeddings) for semantic search.\n",
        "* **Google GenAI:** The SDK for accessing Google's Gemini models (specifically Gemini 2.0 Flash).\n",
        "* **Streamlit & Pyngrok:** Frameworks for building the web user interface and creating a secure tunnel to host it from Colab.\n",
        "* **Datasets & Scikit-Learn:** Used to load the HaluEval benchmark dataset and calculate performance metrics like F1-Score and Precision.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ux6vE065YFGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lvLbf8DalhX"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q PyMuPDF spacy neo4j sentence-transformers networkx matplotlib langchain langchain-community langchain-text-splitters google-generativeai streamlit pyngrok datasets scikit-learn\n",
        "\n",
        "# Download English language model for SpaCy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports, Auth, & Model Initialization\n",
        "This cell configures the API connections and loads the AI models into memory.\n",
        "* **LLM:** Gemini 2.5 Flash lite (Chosen for its high speed and massive context window).\n",
        "* **Embeddings:** `all-MiniLM-L6-v2` (An efficient model that creates 384-dimensional vectors).\n",
        "* **Database:** Establishes a secure connection to your Neo4j Aura instance.\n",
        "* **Libraries:** Loads visualization (`matplotlib`, `networkx`) and data handling (`pandas`) libraries.\n"
      ],
      "metadata": {
        "id": "5peL77ZFX9UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from google.colab import files\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import google.generativeai as genai\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "import re\n",
        "import difflib\n",
        "\n",
        "# ==========================================\n",
        "# PASTE YOUR KEYS HERE\n",
        "# ==========================================\n",
        "HF_TOKEN=\"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\"\n",
        "GOOGLE_API_KEY = \"AIzaSyAseGHwM0koEtxDR39VexjKqXUvhuFaXKw\"\n",
        "NEO4J_URI = \"neo4j+s://0870eeae.databases.neo4j.io\"\n",
        "NEO4J_PASSWORD = \"qLHpDYRUYp48L_C0K6RvVRD0gFn5DBODKaQqWGE_UQs\"\n",
        "# ==========================================\n",
        "\n",
        "# 1. Setup Gemini\n",
        "if GOOGLE_API_KEY.startswith(\"PASTE\"):\n",
        "    print(\"Please enter your Google API Key above.\")\n",
        "else:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    try:\n",
        "        gemini_model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "        print(\"Gemini 2.5 Flash lite Loaded!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Gemini: {e}\")\n",
        "\n",
        "# 2. Setup Embeddings & NLP\n",
        "print(\"â³ Loading Embedding Model & SpaCy...\")\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"NLP Models Loaded!\")\n",
        "\n",
        "# 3. Connect to Graph DB\n",
        "if NEO4J_PASSWORD.startswith(\"PASTE\"):\n",
        "    print(\"Please enter your Neo4j Password above.\")\n",
        "else:\n",
        "    try:\n",
        "        driver = GraphDatabase.driver(NEO4J_URI, auth=(\"neo4j\", NEO4J_PASSWORD))\n",
        "        driver.verify_connectivity()\n",
        "        print(\"Connected to Neo4j!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Neo4j Connection Failed: {e}\")"
      ],
      "metadata": {
        "id": "CKe7e9Eza3Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. PDF Ingestion & Text Chunking\n",
        "Handles the upload and preprocessing of the source document.\n",
        "* **Text Extraction:** Uses PyMuPDF to scrape all text content from the uploaded PDF.\n",
        "* **Chunking:** Splits the massive text string into smaller, manageable segments (1000 characters with 200 overlap).\n",
        "* **RecursiveCharacterTextSplitter:** A smart tool that tries to break text at logical points (like paragraphs or sentences) rather than mid-word.\n",
        "\n"
      ],
      "metadata": {
        "id": "gKCpaHsAYkWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the PDF file\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    \"\"\"Extracts raw text from the uploaded PDF.\"\"\"\n",
        "    text = \"\"\n",
        "    with fitz.open(path) as doc:\n",
        "        for page in doc: text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def create_chunks(text, chunk_size=1000, overlap=200):\n",
        "    \"\"\"Splits text into overlapping chunks for retrieval.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "# Process the PDF\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "chunks = create_chunks(pdf_text)\n",
        "print(f\"âœ… Processed {len(chunks)} chunks from {pdf_path}\")"
      ],
      "metadata": {
        "id": "I8qL_a72bwq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Advanced NLP Extraction (Triple Generation)\n",
        "Uses SpaCy dependency parsing to turn unstructured text into structured facts.\n",
        "* **Entity Extraction:** Identifies proper nouns (People, Organizations, Concepts).\n",
        "* **Relation Extraction:** Finds `Subject -> Verb -> Object` patterns in sentences.\n",
        "* **Knowledge Graph Structure:** These triples form the \"nodes\" and \"edges\" of our graph.\n",
        "\n"
      ],
      "metadata": {
        "id": "jhRoPdLBY72z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_structured_data(text_chunk):\n",
        "    \"\"\"\n",
        "    Uses SpaCy to extract Entities and S-V-O triples.\n",
        "    Returns: list of entities, list of (Subject, Verb, Object) tuples\n",
        "    \"\"\"\n",
        "    doc = nlp(text_chunk)\n",
        "    # Extract Entities (excluding numbers like \"1\", \"2023\")\n",
        "    entities = list({ent.text.strip() for ent in doc.ents if ent.label_ not in [\"CARDINAL\", \"ORDINAL\"]})\n",
        "    triples = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Find verbs that act as connectors\n",
        "        if token.pos_ == \"VERB\":\n",
        "            subj = \"\"\n",
        "            obj = \"\"\n",
        "            # Look for subjects and objects in the dependency tree\n",
        "            for child in token.children:\n",
        "                if child.dep_ in (\"nsubj\", \"nsubjpass\"): subj = child.text\n",
        "                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"): obj = child.text\n",
        "\n",
        "            if subj and obj:\n",
        "                # Clean up the verb (lemma) for the relationship type\n",
        "                triples.append((subj, token.lemma_.upper(), obj))\n",
        "\n",
        "    return entities, triples"
      ],
      "metadata": {
        "id": "pJvQH_itHXOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Graph Database Ingestion\n",
        "Populates the Neo4j database with our hybrid data structure.\n",
        "* **Database Cleaning:** Wipes any existing data to ensure a fresh start.\n",
        "* **Vector Index:** Creates a `chunkVectorIndex` to enable similarity search on text vectors.\n",
        "* **Hybrid Storage:** Stores both the text chunks (for RAG) and the extracted entities (for the Graph) and links them together.\n"
      ],
      "metadata": {
        "id": "LAxy5V-SaxXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_chunks_to_neo4j(driver, doc_name, text_chunks):\n",
        "    \"\"\"\n",
        "    1. Wipes database\n",
        "    2. Creates Vector Index\n",
        "    3. Ingests Chunks (Vector) and Entities (Graph)\n",
        "    \"\"\"\n",
        "    with driver.session() as session:\n",
        "        # 1. Clean DB\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "        # 2. Create Vector Index\n",
        "        try:\n",
        "            session.run(\"DROP INDEX chunkVectorIndex IF EXISTS\")\n",
        "            session.run(\"\"\"\n",
        "                CREATE VECTOR INDEX chunkVectorIndex IF NOT EXISTS\n",
        "                FOR (c:Chunk) ON (c.embedding)\n",
        "                OPTIONS {indexConfig: { `vector.dimensions`: 384, `vector.similarity_function`: 'cosine' }}\n",
        "            \"\"\")\n",
        "            print(\"Vector Index Created.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Index creation note: {e}\")\n",
        "\n",
        "        # 3. Ingest Data\n",
        "        session.run(\"MERGE (d:Document {name: $name})\", name=doc_name)\n",
        "        print(f\"Ingesting {len(text_chunks)} chunks...\")\n",
        "\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            emb = embedder.encode(chunk).tolist()\n",
        "            ents, trips = extract_structured_data(chunk)\n",
        "\n",
        "            # Create Chunk Node\n",
        "            session.run(\"\"\"\n",
        "                MATCH (d:Document {name: $doc})\n",
        "                MERGE (c:Chunk {chunkId: $i})\n",
        "                SET c.text = $txt, c.embedding = $emb\n",
        "                MERGE (d)-[:CONTAINS]->(c)\n",
        "            \"\"\", doc=doc_name, i=i, txt=chunk, emb=emb)\n",
        "\n",
        "            # Create Entity Relationships (The Knowledge Graph)\n",
        "            for s, v, o in trips:\n",
        "                 # Sanitize verb for relationship type (replace spaces/dashes with underscores)\n",
        "                 v_clean = v.strip().upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "                 # Only create if verb is valid alphanumeric\n",
        "                 if v_clean.replace(\"_\", \"\").isalnum():\n",
        "                     session.run(f\"\"\"\n",
        "                        MERGE (a:Entity {{name: $s}})\n",
        "                        MERGE (b:Entity {{name: $o}})\n",
        "                        MERGE (a)-[:{v_clean}]->(b)\n",
        "                     \"\"\", s=s, o=o)\n",
        "\n",
        "# Run Ingestion\n",
        "ingest_chunks_to_neo4j(driver, pdf_path, chunks)\n",
        "print(\"Ingestion Complete!\")"
      ],
      "metadata": {
        "id": "nwwKWxZzHXbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Graph Visualization (Knowledge Graph View)\n",
        "Queries the database to generate a visual map of the extracted knowledge.\n",
        "* **Cypher Query:** Fetches nodes and relationships from Neo4j.\n",
        "* **NetworkX:** Builds a graph object in Python memory.\n",
        "* **Matplotlib:** Draws the graph with nodes (Concepts) and edges (Actions).\n",
        "\n"
      ],
      "metadata": {
        "id": "OEZHBSOfjoe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_knowledge_graph(driver, limit=50):\n",
        "    \"\"\"\n",
        "    Visualizes the Entity-Relationship-Entity graph to show concepts.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Query specifically for Entity relations (Concept -> Action -> Concept)\n",
        "    query = f\"\"\"\n",
        "    MATCH (n:Entity)-[r]->(m:Entity)\n",
        "    RETURN n.name AS source, type(r) AS relation, m.name AS target\n",
        "    LIMIT {limit}\n",
        "    \"\"\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        results = session.run(query)\n",
        "        data = list(results)\n",
        "\n",
        "    if not data:\n",
        "        print(\"âš ï¸ No Entity relationships found. Check extraction logic.\")\n",
        "        return\n",
        "\n",
        "    # Build NetworkX graph\n",
        "    for record in data:\n",
        "        source = record['source']\n",
        "        target = record['target']\n",
        "        relation = record['relation']\n",
        "        G.add_edge(source, target, label=relation)\n",
        "\n",
        "    # Visualization Settings\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
        "\n",
        "    # Nodes\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue', alpha=0.9)\n",
        "    # Edges\n",
        "    nx.draw_networkx_edges(G, pos, width=2, alpha=0.6, arrowstyle='-|>', arrowsize=20)\n",
        "    # Labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=7, font_family='sans-serif', font_weight='bold')\n",
        "    # Edge Labels (Actions)\n",
        "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, font_color='darkred')\n",
        "\n",
        "    plt.title(f\"Knowledge Graph Extraction (Top {limit} Relationships)\", fontsize=14)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Run Visualization\n",
        "visualize_knowledge_graph(driver)"
      ],
      "metadata": {
        "id": "-NNDKMveHnau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Retrieval Module\n",
        "The search engine of our RAG pipeline.\n",
        "* **Query Embedding:** Converts the user's question into a vector.\n",
        "* **Vector Search:** Queries the Neo4j `chunkVectorIndex` to find the 15 most similar text chunks.\n",
        "* **Context Window:** We retrieve a larger number of chunks (k=15) because Gemini 2.0 has a large context window, allowing us to be more comprehensive.\n"
      ],
      "metadata": {
        "id": "J9x2e1qPkrOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query, k=15):\n",
        "    \"\"\"Retrieves top-k relevant chunks using Vector Search.\"\"\"\n",
        "    q_emb = embedder.encode(query).tolist()\n",
        "\n",
        "    with driver.session() as session:\n",
        "        res = session.run(\"\"\"\n",
        "            CALL db.index.vector.queryNodes('chunkVectorIndex', $k, $emb)\n",
        "            YIELD node, score\n",
        "            RETURN node.text as text, score\n",
        "        \"\"\", k=k, emb=q_emb)\n",
        "        records = list(res)\n",
        "\n",
        "    if not records: return [], 0.0\n",
        "    text_list = [r['text'] for r in records]\n",
        "    return text_list, records[0]['score']"
      ],
      "metadata": {
        "id": "9vHubT0ZcMoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Generation Module (Gemini)\n",
        "Synthesizes the final answer using the retrieved context.\n",
        "* **Prompt Engineering:** Uses a strict system prompt to control the AI's behavior.\n",
        "* **Negative Constraints:** Forces the model to say \"NO_EVIDENCE_FOUND\" if the answer isn't in the text.\n",
        "* **Citation Requirement:** Instructs the model to be verbose and explain its reasoning, which aids the verification step later.\n",
        "\n"
      ],
      "metadata": {
        "id": "XDuxNaJVl6Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, context_list):\n",
        "    \"\"\"\n",
        "    Generates an answer based ONLY on the retrieved context.\n",
        "    Enforces verbose answers to aid verification.\n",
        "    \"\"\"\n",
        "    if not context_list: return \"NO_EVIDENCE_FOUND\"\n",
        "\n",
        "    # Join context chunks\n",
        "    full_context = \"\\n---\\n\".join(context_list)\n",
        "\n",
        "    prompt = f\"\"\"You are a research assistant.\n",
        "    Answer the question based ONLY on the provided context.\n",
        "\n",
        "    Requirements:\n",
        "    1. Your answer must be detailed (at least one full sentence).\n",
        "    2. Do NOT provide one-word answers like \"Yes\" or \"No\". Explain WHY based on the text.\n",
        "    3. If the answer is not in the context, say exactly 'NO_EVIDENCE_FOUND'.\n",
        "\n",
        "    Context:\n",
        "    {full_context[:20000]} # Truncate to fit context window\n",
        "\n",
        "    Question: {query}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        return gemini_model.generate_content(prompt).text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {e}\""
      ],
      "metadata": {
        "id": "0P6aLM5Al0NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Verification Module (The Sleuth)\n",
        "The core logic for checking facts. It uses two layers of verification:\n",
        "\n",
        "1.  **Vector-Based Semantic Check:** We measure \"Semantic Similarity\" using **Vector Embeddings**.\n",
        "    * *How:* We convert the text into numbers (vectors) and calculate their cosine similarity.\n",
        "    * *Why:* This determines if the *meaning* of the answer matches the evidence, even if the exact words are different.\n",
        "2.  **Hard Entity Veto:** We use SpaCy to extract **Proper Nouns** and **Numbers**.\n",
        "    * *Why:* If the answer contains a specific name or number that is NOT in the source text, we penalize the score immediately. This catches \"confident lies\" that might otherwise sound semantically similar."
      ],
      "metadata": {
        "id": "m7czpmlNfJ89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_answer(question, answer, context_list):\n",
        "    \"\"\"\n",
        "    Returns: Score (float), Reason (str)\n",
        "    Logic: Boosted Vector Similarity + Hard Entity Veto\n",
        "    \"\"\"\n",
        "    # 1. Handle Refusals\n",
        "    if \"NO_EVIDENCE\" in answer or \"I cannot answer\" in answer:\n",
        "        return 0.1, \"Model Refusal\"\n",
        "\n",
        "    # 2. Vector Similarity (Best Match Strategy)\n",
        "    ans_vec = embedder.encode(answer)\n",
        "    best_sem_score = 0.0\n",
        "\n",
        "    # Check against ALL chunks, find the single best matching sentence/chunk\n",
        "    for chunk in context_list:\n",
        "        ctx_vec = embedder.encode(chunk)\n",
        "        # Cosine similarity\n",
        "        score = np.dot(ans_vec, ctx_vec) / (np.linalg.norm(ans_vec) * np.linalg.norm(ctx_vec))\n",
        "        if score > best_sem_score:\n",
        "            best_sem_score = score\n",
        "\n",
        "    # CALIBRATION: Boost low raw scores to utilize the full 0-1 range\n",
        "    boosted_vec_score = (best_sem_score - 0.2) / 0.6\n",
        "    boosted_vec_score = max(0.0, min(1.0, boosted_vec_score))\n",
        "\n",
        "    # 3. The \"Hard Veto\" Check (Entity Verification)\n",
        "    doc_ans = nlp(answer)\n",
        "\n",
        "    # Identify Critical Terms (Names, Numbers) vs Generic Nouns\n",
        "    critical_terms = [t.text.lower() for t in doc_ans if t.pos_ in [\"PROPN\", \"NUM\"]]\n",
        "    generic_terms = [t.lemma_.lower() for t in doc_ans if t.pos_ in [\"NOUN\"] and t.text.lower() not in critical_terms]\n",
        "\n",
        "    full_source = \" \".join(context_list).lower()\n",
        "\n",
        "    # Veto Logic\n",
        "    missing_critical = [t for t in critical_terms if t not in full_source]\n",
        "\n",
        "    if missing_critical:\n",
        "        # HARD PENALTY: Hallucinated Entity/Number detected\n",
        "        final_score = 0.25\n",
        "        reason = f\"Hallucination Detected: The terms {missing_critical} are not in the source text.\"\n",
        "    else:\n",
        "        # Check overlap of generic nouns\n",
        "        missing_generic = [t for t in generic_terms if t not in full_source]\n",
        "        if not generic_terms:\n",
        "            overlap_ratio = 1.0\n",
        "        else:\n",
        "            overlap_ratio = 1.0 - (len(missing_generic) / len(generic_terms))\n",
        "\n",
        "        # Weighted Average Formula\n",
        "        final_score = (boosted_vec_score * 0.4) + (overlap_ratio * 0.6)\n",
        "        reason = f\"Vector: {best_sem_score:.2f} | Match: {overlap_ratio:.2f}\"\n",
        "\n",
        "    return float(final_score), reason"
      ],
      "metadata": {
        "id": "v8xVzhprmCjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Backend Test Suite\n",
        "Performs a sanity check on the pipeline before running full benchmarks.\n",
        "* **Manual Queries:** Runs 3 specific questions known to test different capabilities.\n",
        "* **Tests Structure:** Checks if the system can list components correctly.\n",
        "* **Tests Refusal:** Checks if the system correctly says \"NO_EVIDENCE\" for a fake question.\n",
        "* **Tests Stats:** Checks if the system can verify numerical data.\n"
      ],
      "metadata": {
        "id": "SM-EL6mbmKLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pipeline(query):\n",
        "    print(f\"\\nðŸ”Ž {query}\")\n",
        "    ctx_list, score = retrieve_context(query)\n",
        "    if not ctx_list: return\n",
        "\n",
        "    ans = generate_answer(query, ctx_list)\n",
        "    print(f\"ðŸ¤– {ans}\")\n",
        "\n",
        "    final_score, reason = verify_answer(query, ans, ctx_list)\n",
        "\n",
        "    # Preliminary threshold check (0.5)\n",
        "    tag = \"âœ… SUPPORTED\" if final_score > 0.5 else \"âš ï¸ FLAGGED\"\n",
        "\n",
        "    print(f\"ðŸ“Š Score: {final_score:.4f} | {tag}\")\n",
        "    print(f\"   Details: {reason}\")\n",
        "\n",
        "# Run manual tests (Ensure these match your PDF content)\n",
        "test_pipeline(\"What are the three modules of the SayNav framework?\")\n",
        "test_pipeline(\"What specific robot platform (e.g., TurtleBot, Boston Dynamics Spot) was used for the real-world cafeteria demonstration?\")\n",
        "test_pipeline(\"What is the success rate of SayNav compared to the baseline?\")"
      ],
      "metadata": {
        "id": "IREZjNEomMTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Threshold Tuning (Calibration)\n",
        "Scientifically determines the best cutoff score for \"Supported\" vs \"Hallucinated.\"\n",
        "* **HaluEval Dataset:** Loads 1,000 samples of real questions with both right and wrong answers.\n",
        "* **Grid Search:** Tests threshold values from 0.1 to 0.95.\n",
        "* **Optimization Goal:** Finds the threshold that maximizes the F1-Score (balancing Precision and Recall).\n",
        "."
      ],
      "metadata": {
        "id": "HvYq0Qty_XuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_threshold(num_samples=1000):\n",
        "    \"\"\"\n",
        "    Runs a grid search on a calibration subset to find the\n",
        "    threshold that maximizes F1 Score.\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ§ª Tuning Threshold on {num_samples} calibration samples...\")\n",
        "\n",
        "    # 1. Load Calibration Data\n",
        "    try:\n",
        "        # Load dataset and shuffle\n",
        "        dataset = load_dataset(\"pminervini/HaluEval\", \"qa\", split=\"data\", trust_remote_code=True)\n",
        "        samples = dataset.shuffle(seed=42).select(range(num_samples))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Dataset Error: {e}\")\n",
        "        return 0.5 # Default fallback\n",
        "\n",
        "    # 2. Generate Scores\n",
        "    results = []\n",
        "    print(\"â³ Generating calibration scores...\")\n",
        "    for row in tqdm(samples):\n",
        "        ctx = row['knowledge']\n",
        "\n",
        "        # Positive Case (Right Answer) - Should pass\n",
        "        s_pos, _ = verify_answer(row['question'], row['right_answer'], [ctx])\n",
        "        results.append({'score': s_pos, 'label': 1})\n",
        "\n",
        "        # Negative Case (Hallucinated Answer) - Should fail\n",
        "        s_neg, _ = verify_answer(row['question'], row['hallucinated_answer'], [ctx])\n",
        "        results.append({'score': s_neg, 'label': 0})\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # 3. Grid Search (0.1 to 0.95)\n",
        "    best_t = 0.5\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    for t in np.arange(0.1, 0.95, 0.05):\n",
        "        y_pred = (df['score'] >= t).astype(int)\n",
        "        f1 = f1_score(df['label'], y_pred)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "\n",
        "    print(f\"\\nðŸ† Optimal Threshold Found: {best_t:.2f}\")\n",
        "    print(f\"   (Calibration F1-Score: {best_f1:.2%})\")\n",
        "\n",
        "    return best_t\n",
        "\n",
        "# Execute and save globally\n",
        "OPTIMAL_THRESHOLD = find_optimal_threshold(1000)"
      ],
      "metadata": {
        "id": "Lvb9o6hF_YVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Final Quantitative Evaluation\n",
        "Generates the final performance report for the project.\n",
        "* **Unseen Data:** Runs the benchmark on a *fresh* set of 1,000 samples (different seed).\n",
        "* **Metrics:** Calculates Accuracy, Precision, Recall, and F1-Score.\n",
        "* **Confusion Matrix:** Shows exactly how many True Positives, False Positives, etc., were found.\n"
      ],
      "metadata": {
        "id": "Djzr9Ldr0fDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_final_evaluation(threshold, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Runs the final performance test using the fixed threshold.\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸš€ Running Final Evaluation on {num_samples} samples...\")\n",
        "    print(f\"âš™ï¸ Locked Threshold: {threshold:.2f}\")\n",
        "\n",
        "    try:\n",
        "        dataset = load_dataset(\"pminervini/HaluEval\", \"qa\", split=\"data\", trust_remote_code=True)\n",
        "        # Use a DIFFERENT seed (999) to ensure we test on unseen data\n",
        "        test_samples = dataset.shuffle(seed=999).select(range(num_samples))\n",
        "    except:\n",
        "        return\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    print(\"â³ Evaluating...\")\n",
        "    for row in tqdm(test_samples):\n",
        "        ctx = row['knowledge']\n",
        "\n",
        "        # 1. Test Faithful Answer (True Positive)\n",
        "        s1, _ = verify_answer(row['question'], row['right_answer'], [ctx])\n",
        "        pred1 = 1 if s1 >= threshold else 0\n",
        "        y_true.append(1)\n",
        "        y_pred.append(pred1)\n",
        "\n",
        "        # 2. Test Hallucinated Answer (True Negative)\n",
        "        s0, _ = verify_answer(row['question'], row['hallucinated_answer'], [ctx])\n",
        "        pred0 = 1 if s0 >= threshold else 0\n",
        "        y_true.append(0)\n",
        "        y_pred.append(pred0)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "    print(f\"\\nðŸ“Š === FINAL PERFORMANCE REPORT ===\")\n",
        "    print(f\" Threshold Used: {threshold:.2f}\")\n",
        "    print(f\"-----------------------------------\")\n",
        "    print(f\"âœ… Accuracy:  {acc:.1%}\")\n",
        "    print(f\"ðŸŽ¯ Precision: {prec:.1%}\")\n",
        "    print(f\"ðŸ”Ž Recall:    {rec:.1%}\")\n",
        "    print(f\"âš–ï¸ F1-Score:  {f1:.1%}\")\n",
        "    print(f\"-----------------------------------\")\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(f\" TP (Correctly Verified):      {tp}\")\n",
        "    print(f\" TN (Caught Hallucinations):   {tn}\")\n",
        "    print(f\" FP (False Alarm):             {fp}\")\n",
        "    print(f\" FN (Missed Valid):            {fn}\")\n",
        "\n",
        "# Run Evaluation\n",
        "run_final_evaluation(OPTIMAL_THRESHOLD, 1000)"
      ],
      "metadata": {
        "id": "RQRqoS5Z0gNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Streamlit App Generation\n",
        "Writes the full Python application code (`app.py`) to disk.\n",
        "* **Integrated Logic:** Combines ingestion, retrieval, generation, and verification into one file.\n",
        "* **User Interface:** Creates tabs for \"Chat\" and \"Knowledge Graph.\"\n",
        "* **Real-time Feedback:** Displays color-coded verification badges (Green/Red) and allows users to expand details to see *why* an answer was flagged.\n"
      ],
      "metadata": {
        "id": "mADASLgqubJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from neo4j import GraphDatabase\n",
        "import fitz  # PyMuPDF\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import spacy\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- CONFIG ---\n",
        "st.set_page_config(page_title=\"CitationSleuth\", layout=\"wide\")\n",
        "\n",
        "# ==========================================\n",
        "# PASTE YOUR KEYS HERE\n",
        "# ==========================================\n",
        "GOOGLE_API_KEY = \"AIzaSyAseGHwM0koEtxDR39VexjKqXUvhuFaXKw\"\n",
        "NEO4J_URI = \"neo4j+s://0870eeae.databases.neo4j.io\"\n",
        "NEO4J_PASSWORD = \"qLHpDYRUYp48L_C0K6RvVRD0gFn5DBODKaQqWGE_UQs\"\n",
        "# ==========================================\n",
        "\n",
        "# --- INITIALIZATION ---\n",
        "if \"last_answer\" not in st.session_state:\n",
        "    st.session_state.last_answer = None\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "if GOOGLE_API_KEY and not GOOGLE_API_KEY.startswith(\"PASTE\"):\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    llm = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return nlp, embedder\n",
        "\n",
        "nlp, embedder = load_resources()\n",
        "\n",
        "def get_db():\n",
        "    if NEO4J_PASSWORD.startswith(\"PASTE\"): return None\n",
        "    try: return GraphDatabase.driver(NEO4J_URI, auth=(\"neo4j\", NEO4J_PASSWORD))\n",
        "    except: return None\n",
        "\n",
        "# --- 1. GRAPH VISUALIZATION ---\n",
        "def plot_graph(driver, focus_text=None):\n",
        "    if not driver: return None, \"Database connection failed.\"\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    title = \"Global Knowledge Graph\"\n",
        "\n",
        "    target_names = []\n",
        "    if focus_text:\n",
        "        doc = nlp(focus_text)\n",
        "        target_names = [t.text for t in doc if t.pos_ in [\"PROPN\", \"NOUN\"]]\n",
        "        target_names = list(set(target_names))\n",
        "\n",
        "    with driver.session() as session:\n",
        "        if target_names:\n",
        "            query_specific = \"\"\"\n",
        "                MATCH (n:Entity)-[r]->(m:Entity)\n",
        "                WHERE n.name IN $targets OR m.name IN $targets\n",
        "                RETURN n.name as source, type(r) as relation, m.name as target LIMIT 50\n",
        "            \"\"\"\n",
        "            res = list(session.run(query_specific, targets=target_names))\n",
        "            if res:\n",
        "                title = f\"Context Graph ({len(res)} nodes)\"\n",
        "                for r in res: G.add_edge(r['source'], r['target'], label=r['relation'])\n",
        "\n",
        "        # Fallback to global if empty or no targets\n",
        "        if G.number_of_nodes() == 0:\n",
        "            query_global = \"\"\"\n",
        "                MATCH (n:Entity)-[r]->(m:Entity)\n",
        "                RETURN n.name as source, type(r) as relation, m.name as target LIMIT 40\n",
        "            \"\"\"\n",
        "            res = list(session.run(query_global))\n",
        "            for r in res: G.add_edge(r['source'], r['target'], label=r['relation'])\n",
        "\n",
        "    if G.number_of_nodes() == 0:\n",
        "        return None, \"Graph is empty. Please ingest a PDF first.\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    pos = nx.spring_layout(G, k=0.6, seed=42) # Increased k for better spacing\n",
        "\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='#add8e6', alpha=0.9, ax=ax)\n",
        "    nx.draw_networkx_edges(G, pos, width=1.2, alpha=0.6, arrowstyle='-|>', arrowsize=15, ax=ax)\n",
        "    nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold', font_family='sans-serif', ax=ax)\n",
        "\n",
        "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, font_color='darkred', ax=ax)\n",
        "\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    plt.axis('off')\n",
        "    return fig, \"Success\"\n",
        "\n",
        "# --- 2. VERIFICATION LOGIC (Updated) ---\n",
        "def verify_answer(answer, context_list):\n",
        "    \"\"\"\n",
        "    Returns: Score (float), UI_Message (str)\n",
        "    \"\"\"\n",
        "    if \"NO_EVIDENCE\" in answer:\n",
        "        return 0.01, \"âš ï¸ The model stated it found no evidence.\"\n",
        "\n",
        "    # A. Vector Score\n",
        "    ans_vec = embedder.encode(answer)\n",
        "    best_sem_score = 0.0\n",
        "    best_chunk_idx = 0\n",
        "\n",
        "    for i, chunk in enumerate(context_list):\n",
        "        ctx_vec = embedder.encode(chunk)\n",
        "        score = np.dot(ans_vec, ctx_vec) / (np.linalg.norm(ans_vec) * np.linalg.norm(ctx_vec))\n",
        "        if score > best_sem_score:\n",
        "            best_sem_score = score\n",
        "            best_chunk_idx = i\n",
        "\n",
        "    boosted = max(0.0, min(1.0, (best_sem_score - 0.2) / 0.6))\n",
        "\n",
        "    # B. Entity Veto\n",
        "    doc = nlp(answer)\n",
        "    keys = [t.lemma_.lower() for t in doc if t.pos_ in [\"PROPN\", \"NUM\", \"NOUN\"]]\n",
        "    full_src = \" \".join(context_list).lower()\n",
        "\n",
        "    if not keys:\n",
        "        overlap = 0.5\n",
        "    else:\n",
        "        missed = [t for t in keys if t not in full_src]\n",
        "        overlap = 1.0 - (len(missed) / len(keys))\n",
        "\n",
        "    # C. Final Score\n",
        "    if overlap < 0.5:\n",
        "        final_score = overlap * 0.5\n",
        "    else:\n",
        "        final_score = (boosted * 0.4) + (overlap * 0.6)\n",
        "\n",
        "    # D. Generate UI Message\n",
        "    if final_score > 0.5:\n",
        "        chunk_text = context_list[best_chunk_idx]\n",
        "        return float(final_score), f\"**Evidence found in source:**\\n\\n> {chunk_text}\"\n",
        "    else:\n",
        "        if keys and missed:\n",
        "            return float(final_score), f\"**Hallucination Suspected:** The answer mentions '{', '.join(missed[:3])}', which was NOT found in the text.\"\n",
        "        else:\n",
        "            return float(final_score), \"**Low Confidence:** The answer does not semantically match the provided text.\"\n",
        "\n",
        "# --- 3. INGESTION ---\n",
        "def ingest(file):\n",
        "    driver = get_db()\n",
        "    if not driver: return False\n",
        "\n",
        "    doc = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
        "    text = \"\".join([p.get_text() for p in doc])\n",
        "    chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(text)\n",
        "\n",
        "    with driver.session() as s:\n",
        "        s.run(\"MATCH (n) DETACH DELETE n\")\n",
        "        try: s.run(\"CREATE VECTOR INDEX chunkVectorIndex IF NOT EXISTS FOR (c:Chunk) ON (c.embedding) OPTIONS {indexConfig: { `vector.dimensions`: 384, `vector.similarity_function`: 'cosine' }}\")\n",
        "        except: pass\n",
        "\n",
        "        bar = st.progress(0)\n",
        "        for i, c in enumerate(chunks):\n",
        "            emb = embedder.encode(c).tolist()\n",
        "            s.run(\"MERGE (c:Chunk {id:$i}) SET c.text=$t, c.embedding=$e\", i=i, t=c, e=emb)\n",
        "\n",
        "            doc_nlp = nlp(c)\n",
        "            for token in doc_nlp:\n",
        "                if token.pos_ == \"VERB\":\n",
        "                    sub = [x.text for x in token.children if x.dep_ == \"nsubj\"]\n",
        "                    obj = [x.text for x in token.children if x.dep_ == \"dobj\"]\n",
        "                    if sub and obj:\n",
        "                        rel = token.lemma_.upper().replace(\" \", \"_\")\n",
        "                        if rel.replace(\"_\",\"\").isalnum():\n",
        "                            s.run(f\"MERGE (a:Entity {{name:$s}}) MERGE (b:Entity {{name:$o}}) MERGE (a)-[:{rel}]->(b)\", s=sub[0], o=obj[0])\n",
        "            bar.progress((i+1)/len(chunks))\n",
        "    return True\n",
        "\n",
        "# --- 4. RAG PIPELINE ---\n",
        "def ask(q):\n",
        "    driver = get_db()\n",
        "    if not driver: return \"DB Error\", [], 0.0, \"No DB\"\n",
        "\n",
        "    q_vec = embedder.encode(q).tolist()\n",
        "    with driver.session() as s:\n",
        "        res = list(s.run(\"CALL db.index.vector.queryNodes('chunkVectorIndex', 15, $e) YIELD node RETURN node.text as t\", e=q_vec))\n",
        "\n",
        "    if not res: return \"No context found in PDF.\", [], 0.0, \"No context\"\n",
        "    ctxs = [r['t'] for r in res]\n",
        "\n",
        "    # Prompt ensures long answers\n",
        "    prompt = f\"\"\"You are a research assistant.\n",
        "    Answer the question based ONLY on the provided context.\n",
        "\n",
        "    Requirements:\n",
        "    1. Your answer must be detailed (at least one full sentence).\n",
        "    2. Do NOT provide one-word answers like \"Yes\" or \"No\". Explain WHY based on the text.\n",
        "    3. If the answer is not in the context, say exactly 'NO_EVIDENCE_FOUND'.\n",
        "\n",
        "    Context:\n",
        "    {' '.join(ctxs)[:15000]}\n",
        "\n",
        "    Question: {q}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    try: ans = llm.generate_content(prompt).text.strip()\n",
        "    except: ans = \"Error\"\n",
        "\n",
        "    score, reason = verify_answer(ans, ctxs)\n",
        "    return ans, ctxs, score, reason\n",
        "\n",
        "# --- 5. UI LAYOUT ---\n",
        "st.title(\"ðŸ•µï¸ CitationSleuth\")\n",
        "f = st.sidebar.file_uploader(\"Upload PDF\", type=\"pdf\")\n",
        "if f and st.sidebar.button(\"Ingest\"):\n",
        "    if ingest(f): st.sidebar.success(\"Ingested!\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"Chat\", \"Graph\"])\n",
        "\n",
        "with tab1:\n",
        "    for m in st.session_state.messages:\n",
        "        with st.chat_message(m[\"role\"]):\n",
        "            st.write(m[\"content\"])\n",
        "            if \"score\" in m:\n",
        "                color = \"green\" if m[\"score\"] > 0.5 else \"red\"\n",
        "                st.markdown(f\":{color}[**Score: {m['score']:.2f}**]\")\n",
        "                with st.expander(\"Verification Details\"):\n",
        "                    st.markdown(m[\"reason\"])\n",
        "\n",
        "    q = st.chat_input(\"Ask question...\")\n",
        "    if q:\n",
        "        st.session_state.messages.append({\"role\":\"user\", \"content\":q})\n",
        "        with st.chat_message(\"user\"): st.write(q)\n",
        "\n",
        "        a, c, s, r = ask(q)\n",
        "        st.session_state.last_answer = f\"{q} {a}\"\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.write(a)\n",
        "            color = \"green\" if s > 0.5 else \"red\"\n",
        "            st.markdown(f\":{color}[**Score: {s:.2f}**]\")\n",
        "            with st.expander(\"Verification Details\"):\n",
        "                st.markdown(r)\n",
        "\n",
        "        st.session_state.messages.append({\"role\":\"assistant\", \"content\":a, \"score\":s, \"reason\":r})\n",
        "\n",
        "with tab2:\n",
        "    # FIXED: Graph now auto-renders based on state, no refresh button needed\n",
        "    driver = get_db()\n",
        "    if driver:\n",
        "        # Check if we have a question context, otherwise show global\n",
        "        target = st.session_state.last_answer\n",
        "        st.caption(f\"Visualizing: {'Context of latest Q&A' if target else 'Global Document Structure'}\")\n",
        "\n",
        "        fig, msg = plot_graph(driver, focus_text=target)\n",
        "        if fig:\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.info(msg) # Info is better than error for empty start state\n",
        "    else:\n",
        "        st.error(\"Database not connected.\")"
      ],
      "metadata": {
        "id": "e6Nukt--ud0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Launch via Ngrok\n",
        "Hosts the application and makes it accessible via a public URL.\n",
        "* **Subprocess:** Starts the Streamlit server in the background.\n",
        "* **Tunneling:** Uses Ngrok to expose the local Colab port (8501) to the internet.\n",
        "* **Live Link:** Generates a clickable URL to open the CitationSleuth UI.\n",
        "\n"
      ],
      "metadata": {
        "id": "RnpjoCYeuxCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# PASTE YOUR NGROK TOKEN HERE\n",
        "NGROK_TOKEN = \"35te2rwqXbxP1FQsQtGAT1Thcs0_8EMhoQRjuxLeYauLx9WJ\"\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Give it a moment to start\n",
        "time.sleep(3)\n",
        "\n",
        "try:\n",
        "    # Open the tunnel\n",
        "    public_url = ngrok.connect(8501).public_url\n",
        "    print(f\"ðŸš€ App Live at: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "upf_zQ2ruxpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}